---
title: "Forecasting Heart Disease Risks"
author: "Seif Kungulio"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
description: "An analysis to predict heart disease risks using machine learning techniques."
categories: [Data Science, Machine Learning, Healthcare]
tags: [R, Machine Learning, Healthcare, Data Analysis]
output:
  pdf_document:
    toc: true
    number_sections: true
    highlight: espresso
    include:
      in_header: resources/header.tex
    latex_engine: xelatex
header-includes:
  # Font and color setup
  - \usepackage{xcolor}
  - \definecolor{brand}{HTML}{000000}
  - \definecolor{subbrand}{HTML}{212121}
  - \usepackage{titlesec}
  - \titleformat{\section}{\Large\bfseries\color{brand}}{\thesection}{1em}{}
  - \titleformat{\subsection}{\large\bfseries\color{subbrand}}{\thesubsection}{0.75em}{}
  - \usepackage[most]{tcolorbox}
  - \tcbset{colback=white, colframe=brand, coltitle=white, fonttitle=\bfseries}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhead[L]{\textcolor{brand}{Forecasting Heart Disease Risks}}
  - \fancyhead[R]{\textcolor{subbrand}{Seif H. Kungulio}}
  - \fancyfoot[C]{\thepage}
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=12, fig.height=6)

# Set default CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

#Install the following libraries if required
# List of required packages
required_pkgs <- c("tidyverse", "RCurl", "caret", "randomForest", "formatR",
                   "tidymodels", "vip", "themis", "janitor", "GGally", 
                   "yardstick", "finetune", "workflowsets", "probably", 
                   "discrim", "e1071", "rpart", "corrplot", "GGally", "pROC")

# Install and load each package
for (pkg in required_pkgs) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}

theme_set(theme_minimal(base_size = 12))
options(dplyr.summarise.inform = FALSE)
```

\newpage

This page was deliberately left blank

\newpage

# **Business Understanding**
## Introduction
Cardiovascular diseases remain one of the leading causes of death worldwide, imposing significant clinical and financial burdens on individuals and organizations alike. Early identification of people at risk of developing heart disease is therefore a priority not only in healthcare but also in sectors such as insurance and public health policy. For insurance companies in particular, understanding the likelihood that a policyholder will develop heart disease is crucial for assessing health risks, pricing premiums, and designing preventive wellness programs that promote healthier lifestyles among clients.

This project applies predictive analytics and machine learning techniques to the Heart Disease dataset from the UCI Machine Learning Repository. The dataset contains a diverse range of patient attributes—including demographic, physiological, and clinical indicators—that are known to influence cardiovascular health. By systematically analyzing these variables, the project aims to uncover patterns and risk factors associated with the onset of heart disease.

The broader business objective is to build a data-driven decision support system that can forecast an individual’s probability of developing heart disease. Such a model enables insurers to perform risk stratification, design personalized premium plans, and support preventive health initiatives that lower long-term costs. Through this analytical framework, the project demonstrates how data science can transform raw medical data into actionable business intelligence, fostering both economic efficiency and social impact.

## Problem statement
\begin{tcolorbox}[title=Problem statement]
To develop models for an insurance company using the Heart Disease dataset from the UCI Machine Learning Repository. The goal is to predict the likelihood of a person developing heart disease, which would help the insurance company estimate health risks and adjust premiums accordingly.
\end{tcolorbox}

\newpage

# **Data Understanding**
The dataset contains various features related to patients' health and demographic information. We will explore the dataset to understand its structure and relationships between variables.

## Data description
The Heart Disease dataset from the UCI Machine Learning Repository contains 303 instances and 14 attributes. These attributes include both numerical and categorical variables related to patients' health metrics and demographic information. The target variable indicates the presence or absence of heart disease. These attributes are:

1. `age`: Age of the patient (numeric)
2. `sex`: Gender of the patient (1 = male, 0 = female)
3. `cp`: Chest pain type (categorical: 1-4)
4. `trestbps`: Resting blood pressure (numeric)
5. `chol`: Serum cholesterol (numeric)
6. `fbs`: Fasting blood sugar (1 = true, 0 = false)
7. `restecg`: Resting electrocardiographic results (categorical)
8. `thalach`: Maximum heart rate achieved (numeric)
9. `exang`: Exercise-induced angina (1 = yes, 0 = no)
10. `oldpeak`: ST depression induced by exercise (numeric)
11. `slope`: The slope of the peak exercise ST segment (categorical)
12. `ca`: Number of major vessels (0-3, numeric)
13. `thal`: Thalassemia (categorical: 1 = normal, 2 = fixed defect, 3 = reversible defect)
14. `target`: Heart disease (1 = disease, 0 = no disease)

## Data dictionary
The dataset contains 14 key attributes that are either numerical or categorical.

| Attribute	| Type	| Description	| Constraints/ Rules |
|:----------|:------|:------------|:-------------------|
| `age`	| Numerical	| The age of the patient in years	| Range: 29-77 (based on dataset statistics) |
| `sex`	| Categorical	| The gender of the patient	| Values: 1 = Male, 0 = Female |
| `cp` | Categorical	| Type of chest pain experienced by the patient | 	Values: 1 = Typical angina, 2 = Atypical angina, 3 = Non-anginal pain, 4 = Asymptomatic |
| `trestbps`	| Numerical	| Resting blood pressure of the patient, measured in mmHg	| Range: Typically, between 94 and 200 mmHg |
| `chol` | Numerical	| Serum cholesterol level in mg/dl | Range: Typically, between 126 and 564 mg/dl |
| `fbs`	| Categorical	| Fasting blood sugar level > 120 mg/dl | 	Values: 1 = True, 0 = False |
| `restecg` | Categorical	| Results of the patient’s resting electrocardiogram	| Values: 0 = Normal, 1 = ST-T wave abnormality, 2 = Probable or definite left ventricular hypertrophy |
| `thalach`	| Numerical	| Maximum heart rate achieved during a stress test | Range: Typically, between 71 and 202 bpm |
| `exang`	| Categorical	| Whether the patient experiences exercise-induced angina	| Values: 1 = Yes, 0 = No |
| `oldpeak`	| Numerical	| ST depression induced by exercise relative to rest (an ECG measure)	| Range: 0.0 to 6.2 (higher values indicate more severe abnormalities) |
| `slope`	| Categorical	| Slope of the peak exercise ST segment	| Values: 1 = Upsloping, 2 = Flat, 3 = Downsloping |
| `ca` | Numerical	| Number of major vessels colored by fluoroscopy	| Range: 0-3 |
| `thal` | Categorical	| Blood disorder variable related to thalassemia	| Values: 3 = Normal, 6 = Fixed defect, 7 = Reversible defect |
| `target` | Categorical	| Diagnosis of heart disease	| Values: 0 = No heart disease, 1 = Presence of heart disease |

## Initial observations
* The dataset contains a mix of numerical and categorical variables.
* Some variables may require preprocessing, such as handling missing values and encoding categorical variables.
* Missing Values: Some fields like ca and thal may have missing values or unknown entries (‘?’).
* Data Types: Some categorical variables are encoded numerically and will need to be interpreted correctly during analysis.
* Class Imbalance: Preliminary checks suggest the dataset is relatively balanced between presence and absence of disease, but this will be verified.
* Outliers: Numerical fields such as chol (cholesterol) and trestbps (blood pressure) may have outliers that need to be detected and considered in analysis.

\newpage

# **Data Preparation**
## Data loading
Load the dataset from the UCI website to memory
```{r tidy=TRUE, tidy.opts=list(width.cutoff=80)}
# Load the dataset
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"

# Read the dataset into a dataframe
Heart.df <- read.csv(text = getURL(url), header = FALSE, na.strings = "?")
```

Rename the columns into a meaningful column names
```{r}
colnames(Heart.df) <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", 
                        "thalach", "exang", "oldpeak", "slope", "ca", "thal", "target")
```

Display dimensions of the dataset
```{r}
dim(Heart.df)
```

Display the first six rows of the dataset
```{r}
head(Heart.df)
```

Display the structure of the dataframe
```{r}
glimpse(Heart.df)
```

Display the statistical summary of the dataframe
```{r}
summary(Heart.df)
```

## Data preprocessing
We will preprocess the data by handling missing values, encoding categorical variables, and scaling numerical features.

**Convert binary variables to (0, 1)**

According to the data dictionary, the following attributes should be binary variables: `sex`, `fbs`, `exang`, and `target`. But, some shows to have values besides 0’s and 1’s.
Let’s convert binary variables to (0, 1)
```{r}
Heart.df$target <- ifelse(Heart.df$target > 0, 1, 0)
Heart.df$sex <- ifelse(Heart.df$sex > 0, 1, 0)
Heart.df$fbs <- ifelse(Heart.df$fbs > 0, 1, 0)
Heart.df$exang <- ifelse(Heart.df$exang > 0, 1, 0)
```

**Handle missing values**

Handle missing values in `ca` and `thal` variables using mean/mode imputation.
```{r}
Heart.df$ca[is.na(Heart.df$ca)] <- median(Heart.df$ca, na.rm = TRUE)
Heart.df$ca[Heart.df$ca == "?"] <- median(Heart.df$ca, na.rm = TRUE)
Heart.df$thal[is.na(Heart.df$thal)] <- median(Heart.df$thal, na.rm = TRUE)
Heart.df$ca[Heart.df$thal == "?"] <- median(Heart.df$thal, na.rm = TRUE)
```

Check for missing values if still exist
```{r}
sapply(Heart.df, function(x) sum(is.na(x)))
```

**Handle duplicate entries**

Check for duplicate entries and print them if they exist.
```{r}
dupes <- Heart.df[duplicated(Heart.df) | duplicated(Heart.df, fromLast = TRUE), ]
print(dupes)
```

**Convert categorical variables to factor.**

Define a list of categorical columns with their levels and labels
```{r}
categorical_columns <- list(
  sex = list(levels = c(0, 1), labels = c("Female", "Male")),
  cp = list(levels = c(1, 2, 3, 4), labels = c("Typical Angina", 
                                               "Atypical Angina", "Non-Angina",
                                               "Asymptomatic")),
  fbs = list(levels = c(0, 1), labels = c("False", "True")),
  restecg = list(levels = c(0, 1, 2), labels = c("Normal", "Wave-abnormality", "Probable")),
  exang = list(levels = c(0, 1), labels = c("No", "Yes")),
  slope = list(levels = c(1, 2, 3), labels = c("Upsloping", "Flat", 
                                               "Downsloping")),
  thal = list(levels = c(3, 6, 7), labels = c("Normal", "Fixed Defect", "Reversible")),
  target = list(levels = c(1, 0), labels = c("Yes", "No"))
)
```

Apply the factor transformation using a for-loop.
```{r}
for (col in names(categorical_columns)) {
  Heart.df[[col]] <- factor(Heart.df[[col]], 
                            levels = categorical_columns[[col]]$levels, 
                            labels = categorical_columns[[col]]$labels)
}
```

**Handle outliers in numerical variables**

Apply multiple filters to identify and handle outliers in numerical variables.
```{r}
Heart.df <- Heart.df[Heart.df$age > 40 &
                       Heart.df$trestbps < 170 &
                       Heart.df$chol < 340 &
                       Heart.df$chol > 150 &
                       Heart.df$thalach > 115 &
                       Heart.df$oldpeak < 2.4, ]
```

This section of the analysis performs a crucial data-cleaning step aimed at refining the quality of the dataset before modeling and visualization. The filtering operation applies a set of logical conditions to remove extreme or biologically implausible values from key continuous health indicators such as age, blood pressure, cholesterol, heart rate, and ST depression. By doing so, it ensures that the dataset reflects realistic patient characteristics and minimizes the influence of outliers that could distort statistical interpretation or predictive accuracy.

The first filter, `Heart.df$age > 40`, narrows the focus to patients over 40 years of age. This decision is grounded in clinical reasoning—heart disease is relatively uncommon in younger individuals, and including them could introduce noise rather than insight into cardiovascular risk patterns. The next condition, `Heart.df$trestbps < 170`, restricts resting blood pressure to physiologically typical values, removing excessively high readings that may result from measurement error or rare hypertensive crises.

Similarly, cholesterol values are filtered using two constraints: `Heart.df$chol < 340` and `Heart.df$chol > 150`. This dual boundary ensures that cholesterol readings fall within a realistic clinical range, excluding both unusually low and excessively high values. Extremely high cholesterol levels (above 340 mg/dl) could be outliers due to lab errors or rare genetic conditions, while very low levels (below 150 mg/dl) are equally atypical for this patient population.

The condition `Heart.df$thalach > 115` retains only those patients whose maximum heart rate achieved during exercise falls within a normal performance range. Extremely low thalach values often suggest incomplete stress tests or data entry errors, which could bias the interpretation of cardiovascular efficiency. Finally, `Heart.df$oldpeak < 2.4` removes extreme ST depression values. In clinical terms, oldpeak measures the degree of ST segment depression during exercise, and values beyond 2.4 are uncommon and may represent atypical cardiac events that do not align with general population patterns in the dataset.

Overall, these filters collectively enhance the integrity of the data and the reliability of subsequent analysis. By trimming implausible extremes, the dataset becomes more homogeneous, improving the clarity of boxplots, histograms, and scatterplots generated during exploratory data analysis. Moreover, this targeted filtering supports more stable and interpretable model outcomes by preventing a few extreme observations from disproportionately influencing trends or coefficients. The result is a dataset that better represents realistic health profiles, ultimately strengthening the credibility of insights drawn from the heart disease risk modeling process.


## Helper functions

**Function to create Box plots**
```{r}
HeartDiseaseBoxplot <- function(var1, var2) {
  ggplot(Heart.df, aes(x = .data[[var1]],
                       y = .data[[var2]],
                       fill = .data[[var1]])) +
    geom_boxplot() + 
    labs(title = paste("Boxplot of", var2, "by", var1),
         x = var1, y = var2, fill = "Heart Disease")
}
```

**Function to create Bar plots**
```{r}
HeartDiseaseBar <- function(var) {
  ggplot(Heart.df, aes(x = .data[[var]], fill = target)) +
    geom_bar(position = "dodge") +
    labs(title = paste("Distribution of Heart Disease by", var),
         x = var, fill = "Heart Disease")
}
```

**Function to create Histograms**
```{r}
HeartDiseaseHist <- function(var1) {
  ggplot(Heart.df, aes(x = .data[[var1]], fill = target)) +
    geom_histogram(bins = 15) +
    labs(title = paste("Distribution of", var1),
         x = var1, fill = "Heart Disease")
}
```

**Function to create Scatter plots**
```{r}
HeartDiseaseScatter <- function(point1, point2){
  ggplot(Heart.df, aes(x = .data[[point1]],
                       y = .data[[point2]],
                       color = target)) +
    geom_point(size = 2) +
    geom_smooth(method = "lm", se = FALSE, color = "blue", formula = y ~ x) +
    labs(title = paste("Scatterplot of", point1, "by", point2),
       x = point1, y = point2, color = "Heart Disease")
}
```

## Exploratory data analysis
### Boxplots for numerical variables
I used boxplots to visually examine the distribution of key continuous health indicators — such as age, resting blood pressure (trestbps), cholesterol (chol), maximum heart rate (thalach), and ST depression (oldpeak) — across the binary target variable (Heart Disease: Yes / No).
Boxplots were chosen because they efficiently highlight differences in central tendency (median), variability (IQR), and the presence of potential outliers between patients with and without heart disease.

**Boxplot of Age by Heart Disease**
```{r}
HeartDiseaseBoxplot("target", "age")
```


**Boxplot of Resting Blood Pressure (trestbps) by Heart Disease**
```{r}
HeartDiseaseBoxplot("target", "trestbps")
```


**Boxplot of Cholesterol (chol) by Heart Disease**
```{r}
HeartDiseaseBoxplot("target", "chol")
```


**Boxplot of Maximum Heart Rate Achieved (thalach) by Heart Disease**
```{r}
HeartDiseaseBoxplot("target", "thalach")
```


**Boxplot of ST Depression (oldpeak) by Heart Disease**
```{r}
HeartDiseaseBoxplot("target", "oldpeak")
```


**Overall boxplots observations:**







### Barplots for categorical variables

**Heart disease distribution**
```{r}
ggplot(Heart.df, aes(x=target, fill=target))+
  geom_bar() +
  ggtitle("Distribution of Heart Disease") +
  labs(x = "Heart Disease", fill = "Heart Disease")
```

Visualize distribution of categorical variables by heart disease presence.
```{r}
HeartDiseaseBar("sex")
HeartDiseaseBar("cp")
HeartDiseaseBar("fbs")
HeartDiseaseBar("restecg")
HeartDiseaseBar("exang")
HeartDiseaseBar("slope")
HeartDiseaseBar("thal")
```

### Histograms for Numerical Variables
```{r}
HeartDiseaseHist("age")
HeartDiseaseHist("trestbps")
HeartDiseaseHist("chol")
HeartDiseaseHist("thalach")
HeartDiseaseHist("oldpeak")
```

### Scatterplots for numerical variables
```{r}
HeartDiseaseScatter("age", "oldpeak")
HeartDiseaseScatter("age", "chol")
HeartDiseaseScatter("age", "trestbps")
HeartDiseaseScatter("age", "thalach")
HeartDiseaseScatter("chol", "thalach")
HeartDiseaseScatter("trestbps", "chol")
HeartDiseaseScatter("thalach", "oldpeak")
```

### Pairwise correlation plots
Pairwise correlation plot for numerical variables
```{r}
ggpairs(Heart.df[, c("age", "trestbps", "chol", 
                     "thalach", "oldpeak", "target")], 
        aes(color = target, fill = target))
```

### Correlation matrix
Correlation matrix for numerical variables
```{r}
# Selecting only continuous variables
continuous_vars <- c("age", "trestbps", "chol", "thalach", "oldpeak")
continuous_data <- Heart.df %>% select(all_of(continuous_vars))

# Calculating correlation matrix
correlation_matrix <- cor(continuous_data)

# Plotting the correlation matrix
corrplot(correlation_matrix, method = "circle",
         type = "lower", tl.col = "black")
```

### Class imbalance
```{r}
Heart.df %>% count(target) %>% mutate(pct = n/sum(n))
```

### Split & Resampling
```{r}
set.seed(123)
split <- initial_split(Heart.df, prop = 0.8, strata = target)
train <- training(split);  test <- testing(split)

set.seed(123)
folds <- vfold_cv(train, v = 10, strata = target)
```

### Feature engineering
```{r}
base_recipe <-
  recipe(target ~ ., data = train) %>%
  # Optional domain transforms
  step_mutate(
    age_band = cut(age, breaks = c(0,40,50,60,70,100),
                   labels = c("<40","40-49","50-59","60-69","70+"))
  ) %>%
  step_rm(age_band) %>%  # keep linear age first; reintroduce if helpful
  # Handle rare levels (robust to small data)
  step_other(all_nominal_predictors(), threshold = 0.02) %>%
  # Impute
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  # One-hot encode
  step_dummy(all_nominal_predictors()) %>%
  # Standardize numeric
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())
```

```{r include=FALSE}
knitr::knit_exit()
```

\newpage

# **Modeling**

## Logistic Regression
```{r}
logit_spec <-
  logistic_reg(penalty = tune(), mixture = 1) %>%  # LASSO
  set_engine("glmnet")
```

## Random Forest
```{r}
rf_spec <-
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", importance = "permutation", num.threads = parallel::detectCores()) %>%
  set_mode("classification")
```


## XGBoost
```{r}
xgb_spec <-
  boost_tree(
    trees = 1000, tree_depth = tune(), learn_rate = tune(),
    mtry  = tune(), loss_reduction = tune(), min_n = tune()
  ) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
```

## Workflows
```{r}
logit_wf <- workflow() %>% add_model(logit_spec) %>% add_recipe(base_recipe)
rf_wf    <- workflow() %>% add_model(rf_spec)    %>% add_recipe(base_recipe)
xgb_wf   <- workflow() %>% add_model(xgb_spec)   %>% add_recipe(base_recipe)
```

### Workflow set for parallel tuning
```{r}
wf_set <- workflow_set(
  preproc = list(baseline = base_recipe),
  models  = list(logit = logit_spec, rf = rf_spec, xgb = xgb_spec),
  cross   = FALSE
)
```

### Tuning grids
```{r}
logit_grid <- grid_regular(penalty(range = c(-5, 0)), levels = 20) %>% # 1e-5..1
  mutate(penalty = 10^penalty)
rf_grid    <- grid_latin_hypercube(mtry(range = c(2L, 10L)), min_n(), size = 20)
xgb_grid   <- grid_latin_hypercube(
  tree_depth(), learn_rate(range = c(-3, -0.5)), mtry(range = c(2L,10L)),
  loss_reduction(), min_n(), size = 30
) %>% mutate(learn_rate = 10^learn_rate)

ctrl <- control_grid(save_pred = TRUE, parallel_over = "resamples")
```

```{r}
set.seed(2025)
logit_res <- tune_grid(logit_wf, resamples = folds, grid = logit_grid,
                       metrics = metric_set(roc_auc, pr_auc, brier_class), 
                       control = ctrl)
```

```{r}
set.seed(2025)
rf_res    <- tune_grid(rf_wf,    resamples = folds, grid = rf_grid,
                       metrics = metric_set(roc_auc, pr_auc, brier_class), 
                       control = ctrl)
```

<!-- ```{r} -->
<!-- set.seed(2025) -->
<!-- xgb_res   <- tune_grid(xgb_wf,   resamples = folds, grid = xgb_grid, -->
<!--                        metrics = metric_set(roc_auc, pr_auc, brier_class),  -->
<!--                        control = ctrl) -->
<!-- ``` -->

### Model selection



\newpage

# **Evaluation**



\newpage

# **Deployment**


\newpage

# **Conclusion**



\newpage

# **Session Information**

```{r}
sessionInfo()
```
